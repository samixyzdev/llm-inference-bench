{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Inference-Bench\n",
    "\n",
    "GPU benchmarking toolkit for LLM inference performance evaluation.\n",
    "\n",
    "**Metrics measured:**\n",
    "- Throughput (tokens/second)\n",
    "- Time to First Token (TTFT)\n",
    "- Per-token latency\n",
    "- GPU memory usage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch transformers accelerate bitsandbytes\n!pip install -q pandas matplotlib seaborn tqdm pynvml"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Benchmark Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import gc\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supported models for Colab T4\n",
    "SUPPORTED_MODELS = {\n",
    "    \"tiny\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"phi2\": \"microsoft/phi-2\",\n",
    "    \"gemma-2b\": \"google/gemma-2b\",\n",
    "}\n",
    "\n",
    "# Default prompts\n",
    "DEFAULT_PROMPTS = [\n",
    "    \"Explain the concept of machine learning in simple terms.\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"What are the benefits of using renewable energy sources?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_config(quantization: str) -> Optional[BitsAndBytesConfig]:\n",
    "    \"\"\"Get BitsAndBytes quantization config.\"\"\"\n",
    "    if quantization == \"int8\":\n",
    "        return BitsAndBytesConfig(load_in_8bit=True)\n",
    "    elif quantization == \"int4\":\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_model(model_name: str, quantization: str = \"fp16\"):\n",
    "    \"\"\"Load model with specified quantization.\"\"\"\n",
    "    model_path = SUPPORTED_MODELS.get(model_name, model_name)\n",
    "    print(f\"Loading {model_path} with {quantization} precision...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    quant_config = get_quantization_config(quantization)\n",
    "    model_kwargs = {\"trust_remote_code\": True, \"device_map\": \"auto\"}\n",
    "    \n",
    "    if quantization == \"fp16\":\n",
    "        model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "    elif quant_config:\n",
    "        model_kwargs[\"quantization_config\"] = quant_config\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded on {next(model.parameters()).device}\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_throughput(model, tokenizer, prompts, max_new_tokens=128, \n",
    "                         batch_size=1, num_warmup=2, num_runs=5):\n",
    "    \"\"\"Benchmark throughput (tokens/second).\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Prepare batch\n",
    "    batch_prompts = (prompts * (batch_size // len(prompts) + 1))[:batch_size]\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, \n",
    "                       truncation=True, max_length=512).to(device)\n",
    "    input_tokens = inputs.input_ids.shape[1]\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(num_warmup):\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=max_new_tokens, \n",
    "                              do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Benchmark\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for _ in tqdm(range(num_runs), desc=\"Throughput\"):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens,\n",
    "                                    do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        total_time += time.perf_counter() - start\n",
    "        total_tokens += (outputs.shape[1] - input_tokens) * batch_size\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    avg_tokens = total_tokens / num_runs\n",
    "    \n",
    "    return {\n",
    "        \"tokens_per_second\": avg_tokens / avg_time,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_time_seconds\": avg_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_latency(model, tokenizer, prompt, max_new_tokens=128, \n",
    "                      num_warmup=2, num_runs=10):\n",
    "    \"\"\"Benchmark latency (TTFT and per-token).\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, \n",
    "                       max_length=512).to(device)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(num_warmup):\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=max_new_tokens,\n",
    "                              do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Measure TTFT\n",
    "    ttft_times = []\n",
    "    for _ in tqdm(range(num_runs), desc=\"TTFT\"):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=1, do_sample=False,\n",
    "                              pad_token_id=tokenizer.pad_token_id)\n",
    "        torch.cuda.synchronize()\n",
    "        ttft_times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    # Measure full generation\n",
    "    gen_times = []\n",
    "    tokens_list = []\n",
    "    \n",
    "    for _ in tqdm(range(num_runs), desc=\"Generation\"):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens,\n",
    "                                    do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
    "        torch.cuda.synchronize()\n",
    "        gen_times.append((time.perf_counter() - start) * 1000)\n",
    "        tokens_list.append(outputs.shape[1] - input_length)\n",
    "    \n",
    "    avg_ttft = sum(ttft_times) / len(ttft_times)\n",
    "    avg_gen_time = sum(gen_times) / len(gen_times)\n",
    "    avg_tokens = sum(tokens_list) / len(tokens_list)\n",
    "    \n",
    "    per_token = (avg_gen_time - avg_ttft) / max(avg_tokens - 1, 1)\n",
    "    \n",
    "    return {\n",
    "        \"time_to_first_token_ms\": avg_ttft,\n",
    "        \"per_token_latency_ms\": per_token,\n",
    "        \"total_generation_time_ms\": avg_gen_time,\n",
    "        \"tokens_generated\": int(avg_tokens),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_memory(model, tokenizer, prompt, max_new_tokens=128, batch_size=1):\n",
    "    \"\"\"Benchmark GPU memory usage.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    prompts = [prompt] * batch_size\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True,\n",
    "                       truncation=True, max_length=512).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_new_tokens=max_new_tokens,\n",
    "                          do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
    "    \n",
    "    return {\n",
    "        \"peak_memory_mb\": torch.cuda.max_memory_allocated() / (1024**2),\n",
    "        \"allocated_memory_mb\": torch.cuda.memory_allocated() / (1024**2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_benchmark(model_name, quantization, batch_size=1, max_new_tokens=128, num_runs=5):\n",
    "    \"\"\"Run complete benchmark for a configuration.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Benchmarking: {model_name} ({quantization})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    model, tokenizer = load_model(model_name, quantization)\n",
    "    \n",
    "    print(\"\\n[1/3] Throughput...\")\n",
    "    throughput = benchmark_throughput(model, tokenizer, DEFAULT_PROMPTS, \n",
    "                                      max_new_tokens, batch_size, num_runs=num_runs)\n",
    "    \n",
    "    print(\"\\n[2/3] Latency...\")\n",
    "    latency = benchmark_latency(model, tokenizer, DEFAULT_PROMPTS[0], \n",
    "                                max_new_tokens, num_runs=num_runs)\n",
    "    \n",
    "    print(\"\\n[3/3] Memory...\")\n",
    "    memory = benchmark_memory(model, tokenizer, DEFAULT_PROMPTS[0], \n",
    "                              max_new_tokens, batch_size)\n",
    "    \n",
    "    result = {\n",
    "        \"model_name\": model_name,\n",
    "        \"quantization\": quantization,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"throughput\": throughput,\n",
    "        \"latency\": latency,\n",
    "        \"memory\": memory,\n",
    "    }\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark - TinyLlama with different quantizations\n",
    "results = []\n",
    "\n",
    "for quant in [\"fp16\", \"int8\", \"int4\"]:\n",
    "    try:\n",
    "        result = run_full_benchmark(\"tiny\", quant, num_runs=3)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {quant}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for r in results:\n",
    "    summary_data.append({\n",
    "        \"Model\": r[\"model_name\"],\n",
    "        \"Quantization\": r[\"quantization\"],\n",
    "        \"Throughput (tok/s)\": f\"{r['throughput']['tokens_per_second']:.2f}\",\n",
    "        \"TTFT (ms)\": f\"{r['latency']['time_to_first_token_ms']:.2f}\",\n",
    "        \"Per-Token (ms)\": f\"{r['latency']['per_token_latency_ms']:.2f}\",\n",
    "        \"Peak Memory (GB)\": f\"{r['memory']['peak_memory_mb']/1024:.2f}\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*70)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot throughput comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "labels = [f\"{r['model_name']}\\n({r['quantization']})\" for r in results]\n",
    "throughputs = [r['throughput']['tokens_per_second'] for r in results]\n",
    "\n",
    "bars = plt.bar(labels, throughputs, color=sns.color_palette(\"husl\", len(results)))\n",
    "plt.ylabel(\"Tokens per Second\", fontsize=12)\n",
    "plt.title(\"Throughput Comparison\", fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars, throughputs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{val:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('throughput_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot memory comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "labels = [f\"{r['model_name']}\\n({r['quantization']})\" for r in results]\n",
    "memory = [r['memory']['peak_memory_mb']/1024 for r in results]\n",
    "\n",
    "bars = plt.bar(labels, memory, color=sns.color_palette(\"husl\", len(results)))\n",
    "plt.ylabel(\"Peak Memory (GB)\", fontsize=12)\n",
    "plt.title(\"GPU Memory Usage\", fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars, memory):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{val:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('memory_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latency comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "labels = [f\"{r['model_name']}\\n({r['quantization']})\" for r in results]\n",
    "ttft = [r['latency']['time_to_first_token_ms'] for r in results]\n",
    "per_token = [r['latency']['per_token_latency_ms'] for r in results]\n",
    "\n",
    "x = range(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar([i - width/2 for i in x], ttft, width, label='TTFT (ms)')\n",
    "ax.bar([i + width/2 for i in x], per_token, width, label='Per-Token (ms)')\n",
    "\n",
    "ax.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax.set_title('Latency Breakdown', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('latency_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save to JSON\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "with open(f'benchmark_results_{timestamp}.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Save summary to CSV\n",
    "df.to_csv(f'benchmark_summary_{timestamp}.csv', index=False)\n",
    "\n",
    "print(f\"Results saved!\")\n",
    "print(f\"- benchmark_results_{timestamp}.json\")\n",
    "print(f\"- benchmark_summary_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Try different models**: Change `\"tiny\"` to `\"phi2\"` or add your own HuggingFace model path\n",
    "2. **Test batch scaling**: Modify `batch_size` parameter to see throughput scaling\n",
    "3. **Compare more quantizations**: Add INT8 vs INT4 vs FP16 comparisons\n",
    "4. **Export charts**: Use for README and LinkedIn posts"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}